{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If using Google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1619290677476,
     "user": {
      "displayName": "Alicia Bogatin",
      "photoUrl": "",
      "userId": "10704781009943021166"
     },
     "user_tz": 240
    },
    "id": "2r5RoBaLZZWs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import functools\n",
    "import operator\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up read path, read file, write path, write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrape_project_definitions as spd\n",
    "project_path = spd.project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1619290986822,
     "user": {
      "displayName": "Alicia Bogatin",
      "photoUrl": "",
      "userId": "10704781009943021166"
     },
     "user_tz": 240
    },
    "id": "OxBb5VXixzLD",
    "outputId": "65811f8e-bf59-45f9-c377-feba1a4c4f1f"
   },
   "outputs": [],
   "source": [
    "input_filename = 'Batch_4551511_batch_results.xlsx'\n",
    "read_path = f\"{project_path}/8_hits__leads/{input_filename}\"\n",
    "\n",
    "write_path_generic = f\"{project_path}/9_hits__leads_labeled/\"\n",
    "output_filename_leadtype = 'cleaned_mturk_emails.xlsx'\n",
    "write_path_leadtype = f\"{project_path}/9_hits__leads_labeled/{output_filename_leadtype}\"\n",
    "\n",
    "output_filename_leadtype_turks = 'mturk_checker_emails.xlsx'\n",
    "write_path_turks = f\"{project_path}/9_hits__leads_labeled/{output_filename_leadtype_turks}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqU0neLGfll1"
   },
   "source": [
    "# Read in MTurks dataframe and do preliminary reformatting and unstacking of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7882,
     "status": "ok",
     "timestamp": 1619291042037,
     "user": {
      "displayName": "Alicia Bogatin",
      "photoUrl": "",
      "userId": "10704781009943021166"
     },
     "user_tz": 240
    },
    "id": "1gDEDTUct9Bd",
    "outputId": "0d3de17d-d4b2-4da2-f868-557a0cc7bd20"
   },
   "outputs": [],
   "source": [
    "df0 = pd.read_excel(read_path)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "\n",
    "# # add the anchor presence to the  dataframe - scratch work\n",
    "# prefix = \"Answer.\"\n",
    "# item_column = \"email_1\"\n",
    "# df0['HIT_anchor'] = -1\n",
    "# i=1\n",
    "# for index, row in df0.iterrows():\n",
    "#   anchor_presence = int(bool(re.search('[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}', str(df0.loc[index,prefix+item_column]), re.IGNORECASE)))\n",
    "#   df0.loc[index, 'HIT_anchor'] = anchor_presence\n",
    "\n",
    "# df0.loc[df0['HIT_anchor'] == 0]['Answer.email_1']=\"N/A_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and clean nan values\n",
    "print('df0 nan values: ',df0.isnull().sum().sum())\n",
    "df1 = df0.fillna('N/A_1')\n",
    "print('df1 nan values: ',df1.isnull().sum().sum())\n",
    "\n",
    "# Unstacking of the data side by side \n",
    "df1['copy_id'] = -1\n",
    "df1=df1.reset_index()\n",
    "\n",
    "# # Group by input columns\n",
    "# input_cols = [col for col in df1.columns if 'Input' in col]\n",
    "# dfgroups = df1.groupby(input_cols).size().reset_index().rename(columns={0:'count'})\n",
    "# Group by HITId\n",
    "dfgroups = df1.groupby(['HITId']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "for row_num, row in dfgroups.iterrows():\n",
    "  #targets = [row[col] for col in input_cols]\n",
    "  #filtered_dfs = (df1[input_cols[index]]== target for index, target in enumerate(targets))\n",
    "  #df_curr_group = df1.loc[functools.reduce(operator.and_, filtered_dfs)]\n",
    "\n",
    "  df_curr_group = df1.loc[ df1['HITId'] == row['HITId']]\n",
    "\n",
    "  ctr = 1\n",
    "  for row_ix, group in df_curr_group.iterrows():\n",
    "    df1.at[row_ix,'copy_id'] = ctr\n",
    "    ctr += 1\n",
    "\n",
    "# Check and clean nan values\n",
    "print('df1 nan values: ',df1.isnull().sum().sum())\n",
    "df2 = df1.set_index(['HITId','copy_id'])\n",
    "print('df2 nan values: ',df2.isnull().sum().sum())\n",
    "#df2.head()\n",
    "\n",
    "df3 = df2.unstack('copy_id')    # you won't always have perfect redundancy so, that will introduce NAN values\n",
    "# df3.head()\n",
    "\n",
    "# Check and clean nan values\n",
    "print('df3 nan values: ',df3.isnull().sum().sum())\n",
    "df4 = df3.fillna('N/A_2')\n",
    "print('df4 nan values: ',df4.isnull().sum().sum())\n",
    "\n",
    "#adjust multiindex level into one level\n",
    "df4.columns = ['_'.join(map(str,col)) for col in df4.columns.values]\n",
    "\n",
    "# Check and clean nan values\n",
    "print('df4 nan values: ',df4.isnull().sum().sum())\n",
    "df5 = df4.reset_index()\n",
    "print('df5 nan values: ',df5.isnull().sum().sum())\n",
    "\n",
    "df5.head()\n",
    "\n",
    "# Removing all unnecessary columns for coming analysis\n",
    "# KEEP ALL INPUT DATA FOR LATER CHECKING (all inputs, all answers, hitid, workerid)\n",
    "\n",
    "relevant_columns = ['HITId'] + [col for col in df5.columns if 'WorkerId' in col] + [col for col in df5.columns if 'ApprovalRate' in col] + [col for col in df5.columns if 'Input' in col] + [col for col in df5.columns if 'Answer' in col]\n",
    "df6 = df5[relevant_columns]\n",
    "\n",
    "# Check and clean nan values\n",
    "print('df6 nan values: ',df6.isnull().sum().sum())\n",
    "df7 = df6.fillna('N/A_3')\n",
    "print('df7 nan values: ',df7.isnull().sum().sum())\n",
    "\n",
    "# [\"-\",\"\",\"n/a\",\"na\",'N/A_1', 'no'] --> you need to convert everything like this to one thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate worker quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workers = pd.DataFrame(columns=['WorkerId', 'total_contribution', 'answers','answer_counts'])\n",
    "\n",
    "workerGroups = df0.groupby(['WorkerId']).size().reset_index().rename(columns={0:'count'})\n",
    "for row_num, row in workerGroups.iterrows():\n",
    "  curr = df0.loc[df0['WorkerId']==row.WorkerId]\n",
    "  curr_new = curr.shape[0]\n",
    "\n",
    "  # count number of non \"N/A\" repeats\n",
    "  duplicates = curr[curr.duplicated(subset='Answer.email_1',keep=False)]\n",
    "  uniques = curr['Answer.email_1'].unique()\n",
    "\n",
    "  if duplicates.shape[0] >= 1:\n",
    "    print('issue')\n",
    "    print(duplicates)\n",
    "\n",
    "    workerId = row['WorkerId']\n",
    "    print(workerId)\n",
    "\n",
    "    print(duplicates['Answer.email_1'].value_counts())\n",
    "    answers = list(duplicates['Answer.email_1'].value_counts().index)\n",
    "    answers_string = \"[\" + \", \".join(answers) +\"]\"\n",
    "    answer_counts = list(duplicates['Answer.email_1'].value_counts())\n",
    "    answer_counts_string = \"[\" + \", \".join([str(x) for x in answer_counts]) +\"]\"\n",
    "\n",
    "    print(answers_string)\n",
    "    print(answer_counts_string)\n",
    "\n",
    "\n",
    "    print(\"duplicates shape: \",str(duplicates.shape[0]))\n",
    "    print(\"uniques shape: \", str(uniques.shape[0]))\n",
    "    print(\"total contribution: \",str(curr.shape[0]))\n",
    "\n",
    "    total_contribution = curr.shape[0]\n",
    "    unique_contributions = uniques.shape[0]\n",
    "    duplicate_contributions = duplicates.shape[0]\n",
    "\n",
    "    df_workers.loc[len(df_workers.index)] = [workerId, total_contribution, answers_string, answer_counts_string] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of workers to ignore/block or not to trust - from manual inspection\n",
    "workers_to_flag = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labelling for the leads dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leads_processing = df7.copy()\n",
    "\n",
    "lead_groups = [['email_1','email_2'],['facebook'],['instagram'],['phone_number'],['website']]\n",
    "lead_groups_anchors = [['^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}$','^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}$'],['facebook\\.com'],['^(@?)[a-zA-Z0-9_][a-zA-Z0-9_.]{2,29}'],['[0-9]{3,11}'],['\\.(com|org|net|edu)']]\n",
    "redundancy = 3 # the redundancy is built into this it will take alot of work to automate this for more or less redundancy\n",
    "prefix = 'Answer.'\n",
    "\n",
    "# sameness_list = ['answer_sameness_'+str(x) for x in [1,2,3]]\n",
    "# anchor_list = ['answer_anchor_'+str(x) for x in [1,2,3]]\n",
    "# decision_list = ['answer_decision_'+str(x) for x in [1,2,3]]\n",
    "# others_list = ['cumulative_sameness','cumulative_anchor','best_guess','runners_up'] # one day you may do a priority ranker\n",
    "# df_leads_processing[sameness_list] = -1 # will be 1/redundancy up to redundancy\n",
    "# df_leads_processing[anchor_list] = -1 # will be 1 or 0\n",
    "# df_leads_processing[decision_list] = -1 # will be 1 or 0 if/when visited\n",
    "# df_leads_processing[others_list] = [-1,-1,'_','_'] \n",
    "\n",
    "# Create labelling for the HITs based on the presence of a regex anchor, and their \"sameness\" with respect to the redundant HITs\n",
    "\n",
    "for lgi in range(len(lead_groups)):\n",
    "  lead_group = lead_groups[lgi]\n",
    "  lead_group_anchors = lead_groups_anchors[lgi]\n",
    "\n",
    "  for i in range(len(lead_group)):\n",
    "    item_column = lead_group[i]\n",
    "    anchor = lead_group_anchors[i]\n",
    "    \n",
    "    df_leads_processing[item_column+\"_best_guess\"] = \"N/A_4\"\n",
    "    df_leads_processing[item_column+\"_runners_up\"] = \"N/A_4\"\n",
    "\n",
    "    for index, row in df_leads_processing.iterrows():\n",
    "\n",
    "      # Make the anchor presence labelling for current item_column\n",
    "      anchor_sum = 0\n",
    "      for i in [1,2,3]: # from redundancy\n",
    "        print(prefix+item_column+str(i))\n",
    "        print(df_leads_processing.loc[index,prefix+item_column+\"_\"+str(i)])\n",
    "        anchor_presence = int(bool(re.fullmatch(anchor, \\\n",
    "            str(df_leads_processing.loc[index,prefix+item_column+\"_\"+str(i)]), re.IGNORECASE))) \n",
    "        df_leads_processing.loc[index, item_column+'_answer_anchor_'+str(i)] = anchor_presence\n",
    "        anchor_sum+=anchor_presence\n",
    "      df_leads_processing.loc[index, item_column+'_cumulative_anchor'] = anchor_sum\n",
    "\n",
    "      # Make the sameness labelling \n",
    "      # This currently only works for redundancy = 3, you have to add sameness group labels for groups larger than 3\n",
    "      temp_dict = {} # (answer_string:sameness)\n",
    "      answer_col_list = [prefix+item_column+\"_\"+str(i) for i in [1,2,3]]\n",
    "      str_list = row[answer_col_list]\n",
    "      for item in str_list:\n",
    "        temp_dict[item]=0\n",
    "      for item in str_list:\n",
    "        temp_dict[item]+=1\n",
    "      sameness_sum = 0\n",
    "      for i in [1,2,3]:\n",
    "        col = answer_col_list[i-1]\n",
    "        val = temp_dict[row[col]]\n",
    "        df_leads_processing.loc[index, item_column+'_answer_sameness_'+str(i)] = val\n",
    "        sameness_sum += val\n",
    "      df_leads_processing.loc[index, item_column+'_cumulative_sameness'] = sameness_sum\n",
    "      \n",
    "\n",
    "      # Based on anchor presence and \"sameness\" values, autoassign the \"best_guess\" value and \"runners_up\" values\n",
    "        # Any viable HIT answers that don't have the maximum sameness value are added to the runners up list\n",
    "\n",
    "        # Need the list of answer_column names\n",
    "        # Need the list of answer_column string values\n",
    "        # Need the list of answer_column sameness values\n",
    "        # Need the list of answer_column anchor values\n",
    "      answer_col_list = [prefix+item_column+\"_\"+str(i) for i in [1,2,3]]\n",
    "      answer_list = [df_leads_processing.loc[index,col] for col in answer_col_list]\n",
    "      sameness_list = [df_leads_processing.loc[index,item_column+\"_answer_sameness_\"+str(i)] for i in [1,2,3]]\n",
    "      anchor_list = [df_leads_processing.loc[index,item_column+\"_answer_anchor_\"+str(i)] for i in [1,2,3]]\n",
    "      sameness_list, answer_col_list, answer_list, anchor_list = zip(*sorted(zip(sameness_list, answer_col_list, answer_list,anchor_list),reverse=True))\n",
    "\n",
    "\n",
    "      flag = False\n",
    "      runners_up = []\n",
    "      visited_answers = []\n",
    "      for i in range(len(sameness_list)):\n",
    "        curr_answer_toCheck = answer_list[i]\n",
    "        curr_answer_anchor = anchor_list[i]\n",
    "\n",
    "        if not flag and curr_answer_anchor == 1:\n",
    "          df_leads_processing.loc[index,item_column+\"_best_guess\"] = curr_answer_toCheck\n",
    "          visited_answers.append(curr_answer_toCheck)\n",
    "          flag = True\n",
    "        elif flag and (curr_answer_anchor == 1) and (curr_answer_toCheck not in visited_answers):         \n",
    "            runners_up.append(curr_answer_toCheck)\n",
    "            visited_answers.append(curr_answer_toCheck)\n",
    "      if len(runners_up)>0:\n",
    "        df_leads_processing.loc[index,item_column+\"_runners_up\"] = \",\".join([str(x) for x in runners_up])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leads_processed = df_leads_processing[['HITId','WorkerId_1'] + [col for col in df_leads_processing.columns if 'Input' in col and '_1' in col] + [col for col in df_leads_processing.columns if \"best_guess\" in col] + [col for col in df_leads_processing.columns if \"runners_up\" in col]  + [col for col in df_leads_processing.columns if \"cumulative\" in col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYXmJfP3WhYp"
   },
   "source": [
    "# Create the final, leads list for each of the lead types (for email will need to connect the two emails columns together), making the database you need (and keeping HITIds, AssignmentIds, and WorkerIds to link the data with other data later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leads_processing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 10948,
     "status": "ok",
     "timestamp": 1618805848893,
     "user": {
      "displayName": "Isaac Howenstine",
      "photoUrl": "",
      "userId": "11451223383071804098"
     },
     "user_tz": 240
    },
    "id": "9NPsYYMD694f",
    "outputId": "c07e58f1-ca15-4f21-9dfe-3f98ff4089d2"
   },
   "outputs": [],
   "source": [
    "# df_emails_large = pd.DataFrame(columns=['university','club','winner_email','flag','list_id'])\n",
    "lead_group_names = ['email','facebook','instagram','phone number','website']\n",
    "lead_groups = [['email_1','email_2'],['facebook'],['instagram'],['phone_number'],['website']]\n",
    "df_leads_list = []\n",
    "df_fileName_list = []\n",
    "blatest = {}\n",
    "\n",
    "for lgi in range(len(lead_groups)):\n",
    "  input_cols = [col for col in df_leads_processed.columns if \"Input\" in col]\n",
    "\n",
    "  lead_group = lead_groups[lgi]\n",
    "  lead_group_name = lead_group_names[lgi]\n",
    "  lead_group_cols = [lead_group_name+\"_best_guess\",lead_group_name+\"_runners_up\", lead_group_name+\"_cumulative_sameness\", lead_group_name+\"_cumulative_anchor\"]\n",
    "  lead_group_df_cols = ['HITId','WorkerId_1'] + input_cols + lead_group_cols + [\"lead_subgroup\"]\n",
    "\n",
    "\n",
    "  # print(input_cols)\n",
    "  # print(lead_group)\n",
    "  # print(lead_group_name)\n",
    "  print(lead_group_df_cols)\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "  lead_group_df = pd.DataFrame(columns=lead_group_df_cols)\n",
    "  print(\"leadgroupdfshape1\")\n",
    "  print(lead_group_df.shape)\n",
    "\n",
    "  # you're gonna loop through all email lead_subgroups\n",
    "  for lead_subgroup in lead_group:\n",
    "    # df_leads_processed_modded = df_leads_processed.rename(columns = lead_group_df_cols)\n",
    "\n",
    "    # print('lead_subgroup: ',lead_subgroup)\n",
    "    lead_subgroup_name = lead_subgroup\n",
    "    # lead_subgroup_name = \n",
    "    lead_subgroup_cols = [lead_subgroup_name+\"_best_guess\",lead_subgroup_name+\"_runners_up\", lead_subgroup_name+\"_cumulative_sameness\", lead_subgroup_name+\"_cumulative_anchor\"]\n",
    "    lead_subgroup_df_cols = ['HITId','WorkerId_1'] + input_cols + lead_subgroup_cols\n",
    "    lead_subgroup_df = df_leads_processed[lead_subgroup_df_cols]\n",
    "    lead_subgroup_df['lead_subgroup'] = lead_subgroup_name\n",
    "    lead_subgroup_df.columns = lead_group_df_cols\n",
    "\n",
    "    lead_group_df = pd.concat([lead_group_df, lead_subgroup_df],  axis=0)\n",
    "    print(\"leadgroupdfshapeupdate\")\n",
    "    print(lead_group_df.shape)\n",
    "\n",
    "  df_fileName_list.append(write_path_generic+\"/\"+\"leads_\"+lead_group_name+\".xlsx\")\n",
    "  df_leads_list.append(lead_group_df)\n",
    "#  print(lead_group_df)\n",
    "\n",
    "'''\n",
    "    print('issue')\n",
    "    print(row[('results','winner_email_1')])\n",
    "    print('index: ',index)\n",
    "    print('\\n')\n",
    "'''\n",
    "    # print(row[('results','winner_email_1')])\n",
    "\n",
    "  # df_emails_large = df_emails_large.fillna('N/A')\n",
    "  # df_emails_large = df_emails_large.loc[df_emails_large['winner_email']!='N/A']\n",
    "  # leads_df_dicts[lead_group_name] = curr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the runners up data into the leads directories for each dataframe and also identify redundancies to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leads_with_runnersUp_list = []\n",
    "for i in range(len(df_leads_list)):\n",
    "  lead_group_name = lead_group_names[i]\n",
    "  df_curr = df_leads_list[i].copy()\n",
    "  print(df_curr.columns)\n",
    "  for index, row in df_curr.iterrows():\n",
    "    rustring = row[lead_group_name+\"_runners_up\"]\n",
    "    runners_up_list = rustring.split(',')\n",
    "\n",
    "    # if len(runners_up_list) > 0:\n",
    "    if \"N/A_4\" not in runners_up_list: \n",
    "      for ru in runners_up_list:\n",
    "        print(ru)\n",
    "        # make a copy of the current row\n",
    "        curr_row_copy = row.copy()\n",
    "        curr_row_copy[lead_group_name+\"_best_guess\"] = ru\n",
    "        df_curr.loc[len(df_curr.index)] = curr_row_copy\n",
    "        print(\"curr row shape: \",str(curr_row_copy.shape))\n",
    "        print(\"df shape: \", str(df_curr.shape))\n",
    "\n",
    "  df_curr[lead_group_name+'_runners_up'] = ''\n",
    "  df_curr['duplicate_lead'] = 0\n",
    "  df_curr.loc[df_curr.duplicated(subset=lead_group_name+'_best_guess',keep=False),'duplicate_lead'] = 1\n",
    "  \n",
    "  df_leads_with_runnersUp_list.append(df_curr)\n",
    "    # for index, row in df_curr.iterrows():\n",
    "    #   row\n",
    "df_leads_with_runnersUp_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4g3vohsWmMi"
   },
   "source": [
    "## Export the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe49Iss_HuHp"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_leads_list)):\n",
    "  # df_leads_list[i].to_excel(df_fileName_list[i])\n",
    "  df_leads_with_runnersUp_list[i].to_excel(df_fileName_list[i])\n",
    "\n",
    "df_workers.to_excel(write_path_turks)\n",
    "df_leads_processed.to_excel(write_path_leadtype)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4.2_mturkDataCleaner.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c5d8f6e74573bcf85721e6caa81fb1e3080928840b56766cb515e33ff1d22f50"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
