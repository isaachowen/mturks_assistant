{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purges previously scraped data from your new scrape, in the case that you are working in multiple pipeline batches. \n",
    "If this is the first scrape in a batch this notebook may not be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If using Google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up read path, read file, write path, write file\n",
    "curr_project_name = \"project_erudition_emails_2\"\n",
    "notebook_path = \"/mnt/c/Users/isaac/Desktop/github_mturks_assistant/mturks_assistant/notebooks\"\n",
    "\n",
    "input_filename='allDATuniversities_northamerica.xlsx'\n",
    "read_path = notebook_path+\"/../\"+input_filename\n",
    "\n",
    "output_filename = 'blast2_universities.xlsx'\n",
    "write_path = notebook_path+\"/../projects/\"+curr_project_name+\"/1_request_data__unis/\"+output_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the directory system (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = [\n",
    "  \"1_request_data__unis\",\n",
    "  \"2_hits__club_lists\",\n",
    "  \"3_hits__club_lists_labeled\",\n",
    "  \"4_request_data__club_lists\",\n",
    "  \"5_hits__clubs\",\n",
    "  \"6_hits__clubs_labeled\",\n",
    "  \"7_request_data__clubs\",\n",
    "  \"8_hits__leads\",\n",
    "  \"9_hits__leads_labeled\",\n",
    "  \"10_leads\",\n",
    "  \"11_leads_accumulator\"\n",
    "]\n",
    "\n",
    "if not os.path.isdir(notebook_path+\"/../projects/\"+curr_project_name):\n",
    "  os.mkdir(notebook_path+\"/../projects/\"+curr_project_name)\n",
    "for d in dir_list:\n",
    "  if not os.path.isdir(notebook_path+\"/../projects/\"+curr_project_name+\"/\"+d):\n",
    "    os.mkdir(notebook_path+\"/../projects/\"+curr_project_name+\"/\"+d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDHha4ahvAi2"
   },
   "source": [
    "## Read in the relevant files \n",
    "### you will need to filter out the unneeded data later (categories/universities that you've already scraped in previous project batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSv8mOMbhyis"
   },
   "outputs": [],
   "source": [
    "df_fullList=pd.read_excel(read_path)\n",
    "\n",
    "# load in the other dataframes\n",
    "previous_input_filenames =['previous_batch_of_universities.xlsx']\n",
    "previous_project_names = [\"project_erudition_emails_1\"]\n",
    "\n",
    "df_dict = {}\n",
    "for project_no in range(len(previous_project_names)):\n",
    "  df_dict[\"df_previous_{0}\".format(project_no)] = pd.read_excel(notebook_path+\"../projects/\"+previous_project_names[project_no]+\"/\"+previous_input_filenames[project_no])\n",
    "  #previous_input_filepath = notebook_path +  + previous_input_filename\n",
    "  #df_previous = pd.read_excel(previous_input_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertically concatenate the other dataframes into one\n",
    "  # Check that the schemas are all the same for the dataframes\n",
    "flag = False\n",
    "for key in pd_dict:\n",
    "  if not (pd_dict[\"df_previous_0\"].columns.intersection(pd_dict[key]) == pd_dict[\"df_previous_0\"].columns)\n",
    "    flag = True\n",
    "    print(key)\n",
    "\n",
    "  # Vertically concatenate the dataframes into one df\n",
    "if not flag:\n",
    "  df_curr = pd.DataFrame().reindex_like(pd_dict[\"df_previous_0\"])\n",
    "  for key in pd_dict:\n",
    "    df_curr = pd.concat([df_curr,pd_dict[key]])\n",
    "\n",
    "df_old = df_curr\n",
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1625781611249,
     "user": {
      "displayName": "Isaac Howenstine",
      "photoUrl": "",
      "userId": "11451223383071804098"
     },
     "user_tz": 240
    },
    "id": "MPDhNIvdEoAS",
    "outputId": "55c7205e-ffdf-4fdb-dadd-123988908f20"
   },
   "outputs": [],
   "source": [
    "df_new = df_fullList.loc[~df_fullList['university_name'].isin(df_old['university_name'])]\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_keUoTRtmEl"
   },
   "outputs": [],
   "source": [
    "df_new.to_excel(write_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMibDf8gmwqWA+Gd+5CXBzh",
   "name": "prep_schools.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c5d8f6e74573bcf85721e6caa81fb1e3080928840b56766cb515e33ff1d22f50"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
