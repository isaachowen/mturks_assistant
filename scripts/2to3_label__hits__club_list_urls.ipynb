{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If using Google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2r5RoBaLZZWs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrape_project_definitions as spd\n",
    "project_path = spd.project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvDntdVzUgXL"
   },
   "outputs": [],
   "source": [
    "input_filename='1_Batch_4329804_batch_results_V2.xlsx'\n",
    "read_path = f\"{project_path}/2_hits__club_lists/{input_filename}\"\n",
    "hit_redundancy = 3\n",
    "\n",
    "output_filename = 'blast2_uni_directories_mod.xlsx'\n",
    "write_path = f\"{project_path}/3_hits__club_lists_labeled/{output_filename}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqU0neLGfll1"
   },
   "source": [
    "## Read in MTurks dataframe and do preliminary reformatting and unstacking of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(read_path)\n",
    "\n",
    "# Present basic profiling information on df1\n",
    "def inspect_dataframe_nulls(df1: pd.DataFrame) -> dict:\n",
    "    nan_counts = df1.isnull().sum()\n",
    "    total_nans = nan_counts.sum()\n",
    "    total_rows = len(df1)\n",
    "\n",
    "    profiling_info = {}\n",
    "    if total_nans > 0:\n",
    "        for col, count in nan_counts.items():\n",
    "            if count > 0:\n",
    "                profiling_info[col] = {}\n",
    "                profiling_info[col]['null_count'] = count\n",
    "                profiling_info[col]['null_percent'] = 100 * count/total_rows\n",
    "    \n",
    "    return profiling_info\n",
    "\n",
    "df1_nulls = inspect_dataframe_nulls(df1)\n",
    "df1_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment ID is the row granularity\n",
    "assert df1.shape[0] == len(pd.unique(df1['AssignmentId']))\n",
    "df1 = df1.dropna(subset=['AssignmentId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the nulls with N/A to enable performing of operations\n",
    "df1 = df1.fillna('N/A')\n",
    "df1_nulls = inspect_dataframe_nulls(df1)\n",
    "assert df1_nulls == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all unnecessary columns for coming analysis\n",
    "general_project_columns = ['Title', 'Description', 'MaxAssignments', 'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds']\n",
    "hit_columns = [\n",
    "    'HITId',\n",
    "    'AssignmentId',\n",
    "    # 'NumberOfSimilarHITs', # I actually don't know what this is\n",
    "    'WorkerId',\n",
    "]\n",
    "input_columns = [col for col in df1.columns if col.startswith('Input.')]\n",
    "answer_columns = [col for col in df1.columns if col.startswith('Answer.')]\n",
    "\n",
    "\n",
    "df2 = df1[hit_columns + input_columns + answer_columns]\n",
    "\n",
    "assert inspect_dataframe_nulls(df2) == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2745,
     "status": "ok",
     "timestamp": 1629908622117,
     "user": {
      "displayName": "Isaac Howenstine",
      "photoUrl": "",
      "userId": "11451223383071804098"
     },
     "user_tz": 300
    },
    "id": "ZxoI64ASXXq-",
    "outputId": "5db4f9c1-3905-432b-b14c-f0a981aa691f"
   },
   "outputs": [],
   "source": [
    "# Unstack the data side by side and add copy_id label in [1,2,3]\n",
    "# The resulting \"primary key\" will be (Input.university_name, Input.city, copy_id)\n",
    "\n",
    "# Another summary attempt:\n",
    "# There are some column names in col_names\n",
    "# In the dataframe, there are multiple rows that contain the same set of strings in those column names\n",
    "# For each of these same set groups, I want to get a view or a loc or a subdataframe of the dataframe so I can do some manipulation on one of the columns of this zoomed in subgroup.\n",
    "\n",
    "# questions about this: what does group_keys = false do... need to learn more about each of the aspects of this calculation\n",
    "df2.loc[:,'copy_id'] = -1\n",
    "def modify_group(group):\n",
    "    \"\"\"Perform operations on each grouped subset.\"\"\"\n",
    "    # You can work with `group` as a DataFrame slice\n",
    "    # Example: Assign unique copy_id within each group\n",
    "    group['copy_id'] = range(1, len(group) + 1)  # Incrementing IDs\n",
    "    \n",
    "    return group  # Return modified group\n",
    "\n",
    "df2 = df2.groupby(input_columns, group_keys=False).apply(modify_group)\n",
    "\n",
    "# Ensure the max copy_id is exactly 3\n",
    "assert df2['copy_id'].max() == 3, f\"Max copy_id is {df2['copy_id'].max()}, expected 3\"\n",
    "\n",
    "# Check that each group has a copy_id == 3\n",
    "for group_name, group in df2.groupby(input_columns):\n",
    "    try:\n",
    "        assert 1 in group['copy_id'].values\n",
    "        assert 2 in group['copy_id'].values\n",
    "        assert 3 in group['copy_id'].values\n",
    "    except AssertionError:\n",
    "        print(f\"Assertion Failed: Group {group_name} is missing copy_id == 3\")\n",
    "        print(f\"Group Indexes: {group.index.tolist()}\")  # Print index of failing rows\n",
    "        print(group)  # Print the problematic group for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fO2tZtNsy63"
   },
   "source": [
    "## Selecting the winning data and making flags for you to manually check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_V3jfSeTSENL"
   },
   "outputs": [],
   "source": [
    "# Idea: you should confirm that the domain of directory link is what you are expecting...how? But all university website are formatted differently, some use external domains for their club directories.\n",
    "# You must be able to quickly and automatically reject HITs if they are not good. Someone took advantage of you. It's the same worker. They could do this because you are basically approving everyone.\n",
    "## Make a worker dataframe. \n",
    "# For each worker: \n",
    "# (1 count how many HITs they do) \n",
    "# (2 confirm they are not using the same link on more than one submission. Flag them if so. You will reject all of their HITs and block them.) \n",
    "# (3 for each HIT done by the worker, check the initial rating of the HIT (using my simple if/else method). Get the cumulative ratings of the HITs and compare to max possible rating, assign this worker rating to the worker. Use this worker trustworthiness rating to re-weight the redundant HITs, see if this forces a different automatic outcome. ) -- is this is a recursive thing? It reminds me of network eigencentrality ratings.\n",
    "# (4 Assemble a global MTurk trustworthiness database based on all my projects. Blacklisted Turks, starred Turks.)\n",
    "\n",
    "# You should include this information to the output you write and will manually check later. It makes it easier to trace back to find out what happened better. It's possible but slow and difficult to trace through everything\n",
    "# In addition to what you already have, write the worker ID, the cumulative input info you gave them from the beginning of the process until now, and the link from the original spreadsheet where the HITs were stored (?) idk what this last thing is\n",
    "\n",
    "# Also, you should make this for an arbitrary level of redundancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhwUi0NG32SE"
   },
   "outputs": [],
   "source": [
    "redundancy = 3 # the redundancy is built into this it will take alot of work to automate this for more or less redundancy\n",
    "item = 'Answer.student_org_url'\n",
    "anchor = '.'\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "  # print(\"index \",index)\n",
    "  # print(\"row item\")\n",
    "  # print(row['HITId'][1])\n",
    "\n",
    "  \n",
    "  # Case 1: All 3 are the same\n",
    "  #if (row['1'+infix+item] == row['2'+infix+item] and row['1'+infix+item] == row['3'+infix+item]):\n",
    "  if (row[item][1] == row[item][2] and row[item][1] == row[item][3]):\n",
    "    #if ( anchor in row['1'+infix+item]):\n",
    "    if ( anchor in row[item][1]):\n",
    "      #row['Winner_'+item] = row['1'+infix+item]\n",
    "      df2.loc[index, ('results','winner_'+item)] = row[item][1]\n",
    "      #row[item+'flag'] = 'All good, triple consensus'\n",
    "      df2.loc[index,('results','flag_'+item)] = 'All good, triple consensus'\n",
    "      #row['Email_1_list'] = ''\n",
    "      df2.loc[index,('results','list_'+item)] = ''\n",
    "    else:\n",
    "      #row['Winner_'+item] = 'N/A'\n",
    "      df2.loc[index,('results','winner_'+item)] = 'N/A'\n",
    "      #row[item+'_flag'] = 'All good, triple consensus'\n",
    "      df2.loc[index,('results','flag_'+item)] = 'All good, triple consensus'\n",
    "      #row[item+'_list'] = ''\n",
    "      df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "### checkpoint\n",
    "\n",
    "  # Case 2: All 3 are different\n",
    "  elif (row[item][1] != row[item][2] and row[item][1] != row[item][3] and row[item][2] != row[item][3]):\n",
    "    templist = []\n",
    "    #print(row[item][1])\n",
    "    #print(row[item][2])\n",
    "    #print(row[item][3])\n",
    "    #print(type(row[item][3]))\n",
    "\n",
    "    if (anchor in row[item][1]):\n",
    "      templist.append(row[item][1])\n",
    "\n",
    "    if (anchor in row[item][2]):\n",
    "      templist.append(row[item][2])\n",
    "\n",
    "    if (anchor in row[item][3]):\n",
    "      templist.append(row[item][3])\n",
    "\n",
    "    if len(templist) == 0:\n",
    "      # row['winner'+item]='N/A'\n",
    "      df2.loc[index,('results','winner_'+item)] = 'N/A'\n",
    "      # row['flag_'+item] = 'All good, zero value'\n",
    "      df2.loc[index,('results','flag_'+item)] = 'All good, zero value'\n",
    "      # row['list_'+item] = ''\n",
    "      df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "    if len(templist) == 1:\n",
    "      # row['winner'+item]=templist[0]\n",
    "      df2.loc[index,('results','winner_'+item)] = templist[0]\n",
    "      # row['flag'+item] = 'Checkit, minority'\n",
    "      df2.loc[index,('results','flag_'+item)] = 'Checkit, minority'\n",
    "      # row['list'+item] = ''\n",
    "      df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "    if len(templist) == 2:\n",
    "      # row['winner'+item] = \",\".join(templist)\n",
    "      df2.loc[index,('results','winner_'+item)] = \",\".join(templist)\n",
    "      # row['flag'+item] = 'Checkit, multiple emails'\n",
    "      df2.loc[index,('results','flag_'+item)] = 'Checkit, multiple emails'\n",
    "      # row['list'+item] = ''\n",
    "      df2.loc[index,('results','list_'+item)] = ''\n",
    "  \n",
    "    if len(templist) == 3:\n",
    "      df2.loc[index,('results','winner_'+item)] = \",\".join(templist)\n",
    "      df2.loc[index,('results','flag_'+item)] = 'Checkit, multiple emails'\n",
    "      df2.loc[index,('results','list_'+item)] = ''\n",
    "      \n",
    "## Checkpoint\n",
    "\n",
    "\n",
    "  # Case 3: 2 are the same and 1 is different\n",
    "  #elif row['1'+infix+item] == row['2'+infix+item]: # implicitly, the odd one out is not equal\n",
    "  elif row[item][1] == row[item][2]: # implicitly, the odd one out is not equal\n",
    "    # if (anchor in row['1'+infix+item]): # there is an anchor in the overlap\n",
    "    if (anchor in row[item][1]): # there is an anchor in the overlap\n",
    "      if (anchor in row[item][3]): # if there is an anchor in the odd one out, we should take a look\n",
    "        #row['winner'+item] = \",\".join([row[item][1],row[item][3]]) #row['1'+infix+item],row['3'+infix+item]])\n",
    "        df2.loc[index,('results','winner_'+item)] = \",\".join([row[item][1],row[item][3]]) #row['1'+infix+item],row['3'+infix+item]])\n",
    "        # row['flag'+item] = 'Checkit, multiple emails'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'Checkit, multiple emails'\n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "      else: # there is no anchor in the odd one out\n",
    "        # row['winner'+item] = row['1'+infix+item]\n",
    "        df2.loc[index,('results','winner_'+item)] = row[item][1]\n",
    "        #row['flag'+item] = 'All good, double consensus'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'All good, double consensus' \n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "    \n",
    "    else: # there is no anchor in the two that overlap\n",
    "      if anchor in row[item][3]: #row['3'+infix+item]: # if there is an anchor in the odd one out, we should take a look\n",
    "        # row['winner'+item] = row['3'+infix+item]\n",
    "        df2.loc[index,('results','winner_'+item)] = row[item][3]\n",
    "        # row['flag'+item] = 'Checkit, minority'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'Checkit, minority'\n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "      else: # there is no anchor in the odd one out or the overlaps\n",
    "        # row['winner'+item] = 'N/A'\n",
    "        df2.loc[index,('results','winner_'+item)] = 'N/A'\n",
    "        # row['flag'+item] = 'All good, zero value'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'All good, zero value'\n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "    # print('hi')\n",
    "  \n",
    "## Checkpoint    \n",
    "  # elif (row['1'+infix+item] == row['3'+infix+item]):\n",
    "  elif (row[item][1] == row[item][3]):\n",
    "  \n",
    "    if (anchor in row[item][1]): # there is an anchor in the overlap\n",
    "      if (anchor in row[item][2]): # if there is an anchor in the odd one out, we should take a look\n",
    "        # row['winner'+item] = ','.join([row['1'+infix+item],row['2'+infix+item]])\n",
    "        df2.loc[index,('results','winner_'+item)] = ','.join([row[item][1],row[item][2]])\n",
    "        # row['flag'+item] = 'Checkit, multiple emails'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'Checkit, multiple emails'\n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "      else: # there is no anchor in the odd one out\n",
    "        # row['winner'+item] = row['1'+infix+item]\n",
    "        df2.loc[index,('results','winner_'+item)] = row[item][1]\n",
    "        \n",
    "        #row['flag'+item] = 'All good, double consensus'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'All good, double consensus'\n",
    "        \n",
    "        #row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "    else: # there is no anchor in the two that overlap\n",
    "      if anchor in row[item][2]: # if there is an @ in the odd one out, we should take a look\n",
    "        \n",
    "        # row['winner'+item] = row['2'+infix+item]\n",
    "        df2.loc[index,('results','winner_'+item)] = row[item][2]\n",
    "        \n",
    "        # row['flag'+item] = 'Checkit, minority'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'Checkit, minority'\n",
    "        \n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "      else: # there is no @ in the odd one out or the overlaps\n",
    "        # row['winner'+item] = 'N/A'\n",
    "        df2.loc[index,('results','winner_'+item)] = 'N/A'\n",
    "\n",
    "        # row['flag'+item] = 'All good, zero value'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'All good, zero value'\n",
    "\n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "    # print('ho')\n",
    "\n",
    "## Checkpoint\n",
    "  #elif (row['2'+infix+item] == row['3'+infix+item]):\n",
    "  elif (row[item][2] == row[item][3]):\n",
    "    \n",
    "    #if (anchor in row['2'+infix+item]): # there is an anchor in the overlap\n",
    "    if (anchor in row[item][2]): # there is an anchor in the overlap\n",
    "\n",
    "      #if (anchor in row['1'+infix+item]): # if there is an anchor in the odd one out, we should take a look\n",
    "      if (anchor in row[item][1]): # if there is an anchor in the odd one out, we should take a look\n",
    "        #row['winner'+item] = ','.join([row['2'+infix+item],row['1'+infix+item]])\n",
    "        df2.loc[index,('results','winner_'+item)] = ','.join([row[item][2],row[item][1]])\n",
    "\n",
    "        #row['flag'+item] = 'Checkit, multiple emails'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'Checkit, multiple emails'\n",
    "\n",
    "        #row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "      else: # there is no anchor in the odd one out\n",
    "        # row['winner'+item] = row['2'+infix+item]\n",
    "        df2.loc[index,('results','winner_'+item)] = row[item][2]\n",
    "        \n",
    "        # row['flag'+item] = 'All good, double consensus'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'All good, double consensus'\n",
    "\n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "## checkpoint    \n",
    "    else: # there is no anchor in the two that overlap\n",
    "      #if anchor in row['1'+infix+item]: # if there is an anchor in the odd one out, we should take a look\n",
    "      if anchor in row[item][1]: #['1'+infix+item]: # if there is an anchor in the odd one out, we should take a look\n",
    "        #row['winner'+item] = row['1'+infix+item]\n",
    "        df2.loc[index,('results','winner_'+item)] = row[item][1]\n",
    "\n",
    "        # row['flag'+item] = 'Checkit, minority'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'Checkit, minority'\n",
    "\n",
    "        # row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "      else: # there is no anchor in the odd one out or the overlaps\n",
    "        #row['winner'+item] = 'N/A'\n",
    "        df2.loc[index,('results','winner_'+item)] = 'N/A'\n",
    "\n",
    "        #row['flag'+item] = 'All good, zero value'\n",
    "        df2.loc[index,('results','flag_'+item)] = 'All good, zero value'\n",
    "\n",
    "        #row['list'+item] = ''\n",
    "        df2.loc[index,('results','list_'+item)] = ''\n",
    "\n",
    "df2 = df2.fillna('N/A')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1629908884479,
     "user": {
      "displayName": "Isaac Howenstine",
      "photoUrl": "",
      "userId": "11451223383071804098"
     },
     "user_tz": 300
    },
    "id": "Cxowe_pBuiBY",
    "outputId": "4630925c-c8b9-4593-f257-59d9a9c5c3a7"
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4g3vohsWmMi"
   },
   "source": [
    "## Export the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe49Iss_HuHp"
   },
   "outputs": [],
   "source": [
    "df2.to_excel(write_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2.2_mturkDataCleaner.ipynb (based on 4.2 colab)",
   "provenance": []
  },
  "interpreter": {
   "hash": "c5d8f6e74573bcf85721e6caa81fb1e3080928840b56766cb515e33ff1d22f50"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
